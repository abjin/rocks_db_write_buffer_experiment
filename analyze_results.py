#!/usr/bin/env python3
"""
RocksDB Write Buffer ìµœì í™” ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ì‘ì„±ì: ì»´í“¨í„°ê³µí•™ê³¼ 4í•™ë…„
ëª©ì : ì‹¤í—˜ ê²°ê³¼ ë°ì´í„° íŒŒì‹±, ë¶„ì„ ë° ì‹œê°í™”
"""

import os
import re
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
from collections import defaultdict
import warnings

warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (ë°œí‘œìš©)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12

class RocksDBResultAnalyzer:
    """RocksDB ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, results_dir="write_buffer_experiment/results"):
        self.results_dir = Path(results_dir)
        self.logs_dir = Path("write_buffer_experiment/logs")
        self.output_dir = Path("write_buffer_experiment/analysis")
        self.output_dir.mkdir(exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥ìš© ë°ì´í„°
        self.results_data = []
        self.summary_stats = {}
        
        print("ğŸ” RocksDB Write Buffer ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ê¸° ì‹œì‘")
        
    def parse_db_bench_output(self, file_path):
        """db_bench ì¶œë ¥ íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ"""
        try:
            with open(file_path, 'r') as f:
                content = f.read()
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path} - {e}")
            return None
            
        metrics = {}
        
        # ì²˜ë¦¬ëŸ‰ (ops/sec) ì¶”ì¶œ
        throughput_match = re.search(r'(\d+)\s+ops/sec', content)
        if throughput_match:
            metrics['throughput'] = int(throughput_match.group(1))
        
        # í‰ê·  ì§€ì—°ì‹œê°„ ì¶”ì¶œ (microseconds)
        avg_latency_match = re.search(r'Average:\s+([\d.]+)', content)
        if avg_latency_match:
            metrics['avg_latency_us'] = float(avg_latency_match.group(1))
            
        # P99 ì§€ì—°ì‹œê°„ ì¶”ì¶œ
        p99_match = re.search(r'Percentiles: P50: ([\d.]+) P95: ([\d.]+) P99: ([\d.]+)', content)
        if p99_match:
            metrics['p50_latency_us'] = float(p99_match.group(1))
            metrics['p95_latency_us'] = float(p99_match.group(2))
            metrics['p99_latency_us'] = float(p99_match.group(3))
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
        mem_match = re.search(r'Block cache size:\s+([\d.]+)\s*MB', content)
        if mem_match:
            metrics['block_cache_mb'] = float(mem_match.group(1))
            
        # Write amplification ì¶”ì¶œ
        wa_match = re.search(r'Write amplification:\s+([\d.]+)', content)
        if wa_match:
            metrics['write_amplification'] = float(wa_match.group(1))
            
        # Compaction í†µê³„ ì¶”ì¶œ
        compaction_match = re.search(r'Compactions\s+Level\s+Files\s+Size.*?\n(.*?)\n', content, re.DOTALL)
        if compaction_match:
            # ê°„ë‹¨í•œ compaction íŒŒì¼ ìˆ˜ ì¶”ì¶œ
            compact_files_match = re.search(r'Total files:\s+(\d+)', content)
            if compact_files_match:
                metrics['total_files'] = int(compact_files_match.group(1))
        
        return metrics
    
    def parse_filename(self, filename):
        """íŒŒì¼ëª…ì—ì„œ ì‹¤í—˜ ì„¤ì • ì •ë³´ ì¶”ì¶œ"""
        # ì˜ˆ: fillrandom_134217728_2_1_iter1.txt
        pattern = r'(\w+)_(\d+)_(\d+)_(\d+)_iter(\d+)\.txt'
        match = re.match(pattern, filename)
        
        if not match:
            return None
            
        benchmark_type = match.group(1)
        write_buffer_size = int(match.group(2))
        max_write_buffer_number = int(match.group(3))
        min_write_buffer_number_to_merge = int(match.group(4))
        iteration = int(match.group(5))
        
        # í¬ê¸°ë¥¼ MBë¡œ ë³€í™˜
        write_buffer_size_mb = write_buffer_size // (1024 * 1024)
        
        return {
            'benchmark_type': benchmark_type,
            'write_buffer_size_bytes': write_buffer_size,
            'write_buffer_size_mb': write_buffer_size_mb,
            'max_write_buffer_number': max_write_buffer_number,
            'min_write_buffer_number_to_merge': min_write_buffer_number_to_merge,
            'iteration': iteration
        }
    
    def load_all_results(self):
        """ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ë° íŒŒì‹±"""
        print("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ ë¡œë”© ì¤‘...")
        
        if not self.results_dir.exists():
            print(f"âŒ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.results_dir}")
            return
            
        result_files = list(self.results_dir.glob("*.txt"))
        print(f"ğŸ“ ì´ {len(result_files)}ê°œì˜ ê²°ê³¼ íŒŒì¼ ë°œê²¬")
        
        for file_path in result_files:
            # íŒŒì¼ëª…ì—ì„œ ì‹¤í—˜ ì„¤ì • ì¶”ì¶œ
            config = self.parse_filename(file_path.name)
            if not config:
                print(f"âš ï¸  íŒŒì¼ëª… íŒŒì‹± ì‹¤íŒ¨: {file_path.name}")
                continue
                
            # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ íŒŒì‹±
            metrics = self.parse_db_bench_output(file_path)
            if not metrics:
                print(f"âš ï¸  ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {file_path.name}")
                continue
            
            # ë°ì´í„° ê²°í•©
            result = {**config, **metrics}
            self.results_data.append(result)
            
        print(f"âœ… {len(self.results_data)}ê°œì˜ ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
        
        # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        if self.results_data:
            self.df = pd.DataFrame(self.results_data)
            print("ğŸ“ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ")
            self.save_raw_data()
        else:
            print("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def save_raw_data(self):
        """ì›ì‹œ ë°ì´í„°ë¥¼ CSV ë° JSONìœ¼ë¡œ ì €ì¥"""
        csv_path = self.output_dir / "experiment_results.csv"
        json_path = self.output_dir / "experiment_results.json"
        
        self.df.to_csv(csv_path, index=False)
        self.df.to_json(json_path, orient='records', indent=2)
        
        print(f"ğŸ’¾ ì›ì‹œ ë°ì´í„° ì €ì¥: {csv_path}, {json_path}")
    
    def calculate_statistics(self):
        """ì‹¤í—˜ ê²°ê³¼ í†µê³„ ê³„ì‚°"""
        print("ğŸ“Š í†µê³„ ë¶„ì„ ì¤‘...")
        
        if self.df.empty:
            print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë°˜ë³µ ì‹¤í—˜ ê²°ê³¼ í‰ê·  ê³„ì‚°
        groupby_cols = ['benchmark_type', 'write_buffer_size_mb', 'max_write_buffer_number', 'min_write_buffer_number_to_merge']
        
        self.summary_stats = self.df.groupby(groupby_cols).agg({
            'throughput': ['mean', 'std'],
            'avg_latency_us': ['mean', 'std'],
            'p99_latency_us': ['mean', 'std'],
            'write_amplification': ['mean', 'std']
        }).round(2)
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        self.summary_stats.columns = ['_'.join(col).strip() for col in self.summary_stats.columns.values]
        self.summary_stats = self.summary_stats.reset_index()
        
        # ìš”ì•½ í†µê³„ ì €ì¥
        summary_path = self.output_dir / "summary_statistics.csv"
        self.summary_stats.to_csv(summary_path, index=False)
        
        print(f"ğŸ“Š ìš”ì•½ í†µê³„ ì €ì¥: {summary_path}")
    
    def create_visualizations(self):
        """ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™”"""
        print("ğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        if self.df.empty:
            return
        
        # ìŠ¤íƒ€ì¼ ì„¤ì •
        sns.set_style("whitegrid")
        
        # 1. Write Buffer Size vs Throughput (fillrandom ê¸°ì¤€)
        self.plot_write_buffer_size_impact()
        
        # 2. ì§€ì—°ì‹œê°„ ë¶„ì„
        self.plot_latency_analysis()
        
        # 3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¶„ì„
        self.plot_memory_efficiency()
        
        # 4. ìµœì  ì¡°í•© ë¹„êµ
        self.plot_optimal_combination()
        
        print("ğŸ¨ ëª¨ë“  ì‹œê°í™” ì™„ë£Œ")
    
    def plot_write_buffer_size_impact(self):
        """Write Buffer Sizeê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Write Buffer Size Impact Analysis', fontsize=16, fontweight='bold')
        
        # fillrandom ê¸°ì¤€ ê¸°ë³¸ ì„¤ì • (2, 1) ë°ì´í„°ë§Œ í•„í„°ë§
        basic_config = self.df[
            (self.df['benchmark_type'] == 'fillrandom') & 
            (self.df['max_write_buffer_number'] == 2) &
            (self.df['min_write_buffer_number_to_merge'] == 1)
        ]
        
        if basic_config.empty:
            print("âš ï¸  fillrandom ê¸°ë³¸ ì„¤ì • ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê·¸ë£¹ë³„ í‰ê·  ê³„ì‚°
        grouped = basic_config.groupby('write_buffer_size_mb').agg({
            'throughput': 'mean',
            'p99_latency_us': 'mean',
            'write_amplification': 'mean'
        }).reset_index()
        
        # 1. Throughput
        ax1.plot(grouped['write_buffer_size_mb'], grouped['throughput'], 
                marker='o', linewidth=2, markersize=8, color='#2E86AB')
        ax1.set_title('Throughput vs Write Buffer Size', fontweight='bold')
        ax1.set_xlabel('Write Buffer Size (MB)')
        ax1.set_ylabel('Throughput (ops/sec)')
        ax1.grid(True, alpha=0.3)
        
        # ìµœì ì  í‘œì‹œ
        max_throughput_idx = grouped['throughput'].idxmax()
        optimal_size = grouped.loc[max_throughput_idx, 'write_buffer_size_mb']
        optimal_throughput = grouped.loc[max_throughput_idx, 'throughput']
        ax1.annotate(f'ìµœì ì : {optimal_size}MB\n{optimal_throughput:,.0f} ops/sec', 
                    xy=(optimal_size, optimal_throughput),
                    xytext=(optimal_size + 50, optimal_throughput + 5000),
                    arrowprops=dict(arrowstyle='->', color='red'),
                    fontsize=10, color='red', fontweight='bold')
        
        # 2. P99 Latency
        ax2.plot(grouped['write_buffer_size_mb'], grouped['p99_latency_us'], 
                marker='s', linewidth=2, markersize=8, color='#A23B72')
        ax2.set_title('P99 Latency vs Write Buffer Size', fontweight='bold')
        ax2.set_xlabel('Write Buffer Size (MB)')
        ax2.set_ylabel('P99 Latency (Î¼s)')
        ax2.grid(True, alpha=0.3)
        
        # 3. Write Amplification
        ax3.plot(grouped['write_buffer_size_mb'], grouped['write_amplification'], 
                marker='^', linewidth=2, markersize=8, color='#F18F01')
        ax3.set_title('Write Amplification vs Write Buffer Size', fontweight='bold')
        ax3.set_xlabel('Write Buffer Size (MB)')
        ax3.set_ylabel('Write Amplification')
        ax3.grid(True, alpha=0.3)
        
        # 4. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± (Throughput per MB)
        grouped['efficiency'] = grouped['throughput'] / grouped['write_buffer_size_mb']
        ax4.bar(grouped['write_buffer_size_mb'], grouped['efficiency'], 
               color='#C73E1D', alpha=0.7)
        ax4.set_title('Memory Efficiency (Throughput per MB)', fontweight='bold')
        ax4.set_xlabel('Write Buffer Size (MB)')
        ax4.set_ylabel('Throughput per MB')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'write_buffer_size_impact.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_latency_analysis(self):
        """ì§€ì—°ì‹œê°„ ìƒì„¸ ë¶„ì„"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # ê¸°ë³¸ ì„¤ì • ë°ì´í„°
        basic_data = self.df[
            (self.df['benchmark_type'] == 'fillrandom') & 
            (self.df['max_write_buffer_number'] == 2) &
            (self.df['min_write_buffer_number_to_merge'] == 1)
        ]
        
        if basic_data.empty:
            return
        
        # 1. Box plot - ì§€ì—°ì‹œê°„ ë¶„í¬
        sns.boxplot(data=basic_data, x='write_buffer_size_mb', y='p99_latency_us', ax=ax1)
        ax1.set_title('P99 Latency Distribution by Buffer Size', fontweight='bold')
        ax1.set_xlabel('Write Buffer Size (MB)')
        ax1.set_ylabel('P99 Latency (Î¼s)')
        
        # 2. ë²¤ì¹˜ë§ˆí¬ íƒ€ì…ë³„ ë¹„êµ
        comparison_data = self.df[
            (self.df['max_write_buffer_number'] == 2) &
            (self.df['min_write_buffer_number_to_merge'] == 1)
        ]
        
        sns.lineplot(data=comparison_data, x='write_buffer_size_mb', y='p99_latency_us', 
                    hue='benchmark_type', marker='o', ax=ax2)
        ax2.set_title('P99 Latency by Benchmark Type', fontweight='bold')
        ax2.set_xlabel('Write Buffer Size (MB)')
        ax2.set_ylabel('P99 Latency (Î¼s)')
        ax2.legend(title='Benchmark Type')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'latency_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_memory_efficiency(self):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¶„ì„"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # ê¸°ë³¸ ì„¤ì • ë°ì´í„°ì—ì„œ íš¨ìœ¨ì„± ê³„ì‚°
        basic_data = self.df[
            (self.df['benchmark_type'] == 'fillrandom') & 
            (self.df['max_write_buffer_number'] == 2) &
            (self.df['min_write_buffer_number_to_merge'] == 1)
        ].copy()
        
        if basic_data.empty:
            return
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (write_buffer_size * max_write_buffer_number)
        basic_data['total_memory_mb'] = basic_data['write_buffer_size_mb'] * basic_data['max_write_buffer_number']
        basic_data['memory_efficiency'] = basic_data['throughput'] / basic_data['total_memory_mb']
        
        # í‰ê·  ê³„ì‚°
        grouped = basic_data.groupby('write_buffer_size_mb').agg({
            'total_memory_mb': 'mean',
            'throughput': 'mean',
            'memory_efficiency': 'mean'
        }).reset_index()
        
        # ì‚°ì ë„
        scatter = ax.scatter(grouped['total_memory_mb'], grouped['throughput'], 
                           s=grouped['memory_efficiency']*1000, 
                           c=grouped['write_buffer_size_mb'], 
                           cmap='viridis', alpha=0.7, edgecolors='black')
        
        # ê° ì ì— ë¼ë²¨ ì¶”ê°€
        for i, row in grouped.iterrows():
            ax.annotate(f'{int(row["write_buffer_size_mb"])}MB', 
                       (row['total_memory_mb'], row['throughput']),
                       xytext=(5, 5), textcoords='offset points',
                       fontsize=10, fontweight='bold')
        
        ax.set_title('Memory Usage vs Throughput\n(Bubble size = Memory Efficiency)', 
                    fontweight='bold', fontsize=14)
        ax.set_xlabel('Total Memory Usage (MB)')
        ax.set_ylabel('Throughput (ops/sec)')
        ax.grid(True, alpha=0.3)
        
        # ì»¬ëŸ¬ë°” ì¶”ê°€
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label('Write Buffer Size (MB)')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'memory_efficiency.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_optimal_combination(self):
        """ìµœì  ì¡°í•© ë¹„êµ (128MB ê¸°ì¤€)"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # 128MBì—ì„œ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì¡°í•© ë°ì´í„°
        optimal_data = self.df[
            (self.df['benchmark_type'] == 'fillrandom') & 
            (self.df['write_buffer_size_mb'] == 128)
        ].copy()
        
        if optimal_data.empty:
            print("âš ï¸  128MB ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì¡°í•©ë³„ í‰ê·  ê³„ì‚°
        optimal_grouped = optimal_data.groupby(['max_write_buffer_number', 'min_write_buffer_number_to_merge']).agg({
            'throughput': 'mean',
            'p99_latency_us': 'mean'
        }).reset_index()
        
        # ì¡°í•© ë¼ë²¨ ìƒì„±
        optimal_grouped['combination'] = optimal_grouped.apply(
            lambda x: f"({int(x['max_write_buffer_number'])}, {int(x['min_write_buffer_number_to_merge'])})", axis=1
        )
        
        # 1. Throughput ë¹„êµ
        bars1 = ax1.bar(optimal_grouped['combination'], optimal_grouped['throughput'], 
                       color='steelblue', alpha=0.7, edgecolor='black')
        ax1.set_title('Throughput by Buffer Configuration\n(Write Buffer Size = 128MB)', fontweight='bold')
        ax1.set_xlabel('(max_write_buffer_number, min_write_buffer_number_to_merge)')
        ax1.set_ylabel('Throughput (ops/sec)')
        ax1.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar in bars1:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height):,}',
                    ha='center', va='bottom', fontweight='bold')
        
        # 2. Latency ë¹„êµ
        bars2 = ax2.bar(optimal_grouped['combination'], optimal_grouped['p99_latency_us'], 
                       color='coral', alpha=0.7, edgecolor='black')
        ax2.set_title('P99 Latency by Buffer Configuration\n(Write Buffer Size = 128MB)', fontweight='bold')
        ax2.set_xlabel('(max_write_buffer_number, min_write_buffer_number_to_merge)')
        ax2.set_ylabel('P99 Latency (Î¼s)')
        ax2.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar in bars2:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}',
                    ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'optimal_combination.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_report(self):
        """ì‹¤í—˜ ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“ ì‹¤í—˜ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        if self.df.empty:
            print("âŒ ë³´ê³ ì„œ ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        report_path = self.output_dir / "experiment_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# RocksDB Write Buffer ìµœì í™” ì‹¤í—˜ ë³´ê³ ì„œ\n\n")
            f.write(f"**ìƒì„±ì¼**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ì‹¤í—˜ ê°œìš”
            f.write("## ì‹¤í—˜ ê°œìš”\n\n")
            f.write(f"- **ì´ ì‹¤í—˜ íšŸìˆ˜**: {len(self.df)}\n")
            f.write(f"- **ë²¤ì¹˜ë§ˆí¬ íƒ€ì…**: {', '.join(self.df['benchmark_type'].unique())}\n")
            f.write(f"- **Write Buffer í¬ê¸° ë²”ìœ„**: {self.df['write_buffer_size_mb'].min()}MB ~ {self.df['write_buffer_size_mb'].max()}MB\n\n")
            
            # ì£¼ìš” ë°œê²¬ì‚¬í•­
            f.write("## ì£¼ìš” ë°œê²¬ì‚¬í•­\n\n")
            
            # ìµœê³  ì„±ëŠ¥ ì„¤ì • ì°¾ê¸°
            best_fillrandom = self.df[self.df['benchmark_type'] == 'fillrandom'].nlargest(1, 'throughput')
            if not best_fillrandom.empty:
                best = best_fillrandom.iloc[0]
                f.write(f"### ìµœê³  ì„±ëŠ¥ ì„¤ì • (fillrandom)\n")
                f.write(f"- **Write Buffer Size**: {best['write_buffer_size_mb']}MB\n")
                f.write(f"- **Max Write Buffer Number**: {best['max_write_buffer_number']}\n")
                f.write(f"- **Min Write Buffer Number To Merge**: {best['min_write_buffer_number_to_merge']}\n")
                f.write(f"- **ì²˜ë¦¬ëŸ‰**: {best['throughput']:,.0f} ops/sec\n")
                f.write(f"- **P99 ì§€ì—°ì‹œê°„**: {best['p99_latency_us']:.1f} Î¼s\n\n")
            
            # í†µê³„ ìš”ì•½
            if hasattr(self, 'summary_stats') and not self.summary_stats.empty:
                f.write("## í†µê³„ ìš”ì•½\n\n")
                f.write("### Write Buffer Sizeë³„ í‰ê·  ì„±ëŠ¥ (fillrandom, ê¸°ë³¸ ì„¤ì •)\n\n")
                
                basic_stats = self.summary_stats[
                    (self.summary_stats['benchmark_type'] == 'fillrandom') &
                    (self.summary_stats['max_write_buffer_number'] == 2) &
                    (self.summary_stats['min_write_buffer_number_to_merge'] == 1)
                ]
                
                f.write("| Buffer Size | Throughput | P99 Latency |\n")
                f.write("|-------------|------------|-------------|\n")
                for _, row in basic_stats.iterrows():
                    f.write(f"| {row['write_buffer_size_mb']}MB | {row['throughput_mean']:,.0f} ops/sec | {row['p99_latency_us_mean']:.1f} Î¼s |\n")
                f.write("\n")
            
            # ê²°ë¡ 
            f.write("## ê²°ë¡ \n\n")
            f.write("1. Write buffer í¬ê¸°ê°€ í´ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ë§Œ, íŠ¹ì • ì§€ì  ì´í›„ë¶€í„°ëŠ” ê°ì†Œ\n")
            f.write("2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì„±ëŠ¥ ê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ í™•ì¸\n")
            f.write("3. ìµœì  ì„¤ì •ì€ ì›Œí¬ë¡œë“œì™€ ì‹œìŠ¤í…œ í™˜ê²½ì— ë”°ë¼ ë‹¬ë¼ì§\n\n")
            
            f.write("## ìƒì„±ëœ íŒŒì¼\n\n")
            f.write("- `experiment_results.csv`: ì›ì‹œ ì‹¤í—˜ ë°ì´í„°\n")
            f.write("- `summary_statistics.csv`: ìš”ì•½ í†µê³„\n")
            f.write("- `*.png`: ë¶„ì„ ê·¸ë˜í”„ë“¤\n")
        
        print(f"ğŸ“‹ ì‹¤í—˜ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_path}")
    
    def run_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        self.load_all_results()
        
        if self.df.empty:
            print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹¤í—˜ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        self.calculate_statistics()
        self.create_visualizations()
        self.generate_report()
        
        print("ğŸ‰ ëª¨ë“  ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ í™•ì¸: {self.output_dir}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ RocksDB Write Buffer ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ì‹œì‘\n")
    
    analyzer = RocksDBResultAnalyzer()
    analyzer.run_analysis()

if __name__ == "__main__":
    main() 