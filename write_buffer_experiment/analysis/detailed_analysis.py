#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RocksDB Write Buffer ìµœì í™” ì‹¤í—˜ ê²°ê³¼ ë¶„ì„
ì‘ì„±ì: ì»´í“¨í„°ê³µí•™ê³¼ 4í•™ë…„
ëª©ì : ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„ ë° ì‹œê°í™”
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
from pathlib import Path

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Malgun Gothic']
plt.rcParams['axes.unicode_minus'] = False

class RocksDBAnalyzer:
    def __init__(self, data_dir="./"):
        self.data_dir = Path(data_dir)
        self.results_df = None
        self.summary_df = None
        self.load_data()
        
    def load_data(self):
        """ì‹¤í—˜ ê²°ê³¼ ë°ì´í„° ë¡œë“œ"""
        try:
            # CSV ë°ì´í„° ë¡œë“œ
            self.results_df = pd.read_csv(self.data_dir / "experiment_results.csv")
            self.summary_df = pd.read_csv(self.data_dir / "summary_statistics.csv")
            print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            print(f"   - ì´ ì‹¤í—˜ íšŸìˆ˜: {len(self.results_df)}")
            print(f"   - ìš”ì•½ í†µê³„: {len(self.summary_df)} ì¡°ê±´")
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            
    def analyze_write_buffer_size_impact(self):
        """Write Buffer Sizeê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸ“Š Write Buffer Size ì˜í–¥ ë¶„ì„")
        print("="*60)
        
        # ê¸°ë³¸ ì„¤ì • (max_write_buffer_number=2, min_write_buffer_number_to_merge=1)ì—ì„œì˜ ê²°ê³¼
        baseline_data = self.results_df[
            (self.results_df['max_write_buffer_number'] == 2) & 
            (self.results_df['min_write_buffer_number_to_merge'] == 1)
        ].copy()
        
        # ë²¤ì¹˜ë§ˆí¬ë³„ ë¶„ì„
        for benchmark in ['fillrandom', 'readrandom', 'overwrite']:
            print(f"\nğŸ” {benchmark.upper()} ë²¤ì¹˜ë§ˆí¬ ë¶„ì„:")
            
            bench_data = baseline_data[baseline_data['benchmark_type'] == benchmark]
            if bench_data.empty:
                continue
                
            # Write buffer sizeë³„ í‰ê·  ì§€ì—°ì‹œê°„
            latency_by_size = bench_data.groupby('write_buffer_size_mb')['avg_latency_us'].agg(['mean', 'std']).round(3)
            print("\nğŸ“ˆ Write Buffer Sizeë³„ í‰ê·  ì§€ì—°ì‹œê°„ (Î¼s):")
            print(latency_by_size)
            
            # ìµœì  ì„±ëŠ¥ ì°¾ê¸°
            best_size = latency_by_size['mean'].idxmin()
            worst_size = latency_by_size['mean'].idxmax()
            
            print(f"\nğŸ† ìµœì  ì„±ëŠ¥: {best_size}MB (í‰ê·  ì§€ì—°ì‹œê°„: {latency_by_size.loc[best_size, 'mean']:.3f}Î¼s)")
            print(f"ğŸ”» ìµœì•… ì„±ëŠ¥: {worst_size}MB (í‰ê·  ì§€ì—°ì‹œê°„: {latency_by_size.loc[worst_size, 'mean']:.3f}Î¼s)")
            
            # ì„±ëŠ¥ ê°œì„ ìœ¨ ê³„ì‚°
            improvement = ((latency_by_size.loc[worst_size, 'mean'] - latency_by_size.loc[best_size, 'mean']) / 
                          latency_by_size.loc[worst_size, 'mean'] * 100)
            print(f"ğŸ“Š ì„±ëŠ¥ ê°œì„ ìœ¨: {improvement:.1f}%")
    
    def analyze_buffer_configuration_impact(self):
        """Buffer ì„¤ì • ì¡°í•©ì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸ”§ Buffer ì„¤ì • ì¡°í•© ì˜í–¥ ë¶„ì„")
        print("="*60)
        
        # 128MBì—ì„œì˜ ë‹¤ì–‘í•œ ì„¤ì • ì¡°í•© ë¶„ì„
        config_data = self.results_df[
            (self.results_df['write_buffer_size_mb'] == 128) & 
            (self.results_df['benchmark_type'] == 'fillrandom')
        ].copy()
        
        if config_data.empty:
            print("âŒ 128MB ì„¤ì • ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # ì„¤ì •ë³„ í‰ê·  ì§€ì—°ì‹œê°„
        config_analysis = config_data.groupby(['max_write_buffer_number', 'min_write_buffer_number_to_merge'])['avg_latency_us'].agg(['mean', 'std', 'count']).round(3)
        
        print("\nğŸ“Š Buffer ì„¤ì •ë³„ ì„±ëŠ¥ (fillrandom, 128MB):")
        print("max_buffers | min_merge | í‰ê· ì§€ì—°ì‹œê°„(Î¼s) | í‘œì¤€í¸ì°¨ | ì‹¤í—˜íšŸìˆ˜")
        print("-" * 65)
        
        for (max_buf, min_merge), stats in config_analysis.iterrows():
            print(f"     {max_buf:2d}     |    {min_merge:2d}    |    {stats['mean']:8.3f}    |  {stats['std']:6.3f}  |    {stats['count']:2.0f}")
        
        # ì´ìƒì¹˜ íƒì§€ (ì§€ì—°ì‹œê°„ì´ ê¸‰ê²©íˆ ì¦ê°€í•˜ëŠ” ê²½ìš°)
        print("\nâš ï¸  ì„±ëŠ¥ ì´ìƒì¹˜ íƒì§€:")
        high_latency = config_analysis[config_analysis['mean'] > 10.0]  # 10Î¼s ì´ìƒ
        if not high_latency.empty:
            for (max_buf, min_merge), stats in high_latency.iterrows():
                print(f"   - max_buffers={max_buf}, min_merge={min_merge}: {stats['mean']:.1f}Î¼s (ì •ìƒ ëŒ€ë¹„ {stats['mean']/2.6:.0f}ë°°)")
        else:
            print("   - ì´ìƒì¹˜ ì—†ìŒ (ëª¨ë“  ì„¤ì •ì´ ì •ìƒ ë²”ìœ„)")
    
    def identify_performance_anomalies(self):
        """ì„±ëŠ¥ ì´ìƒ í˜„ìƒ ì‹ë³„ ë° ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸš¨ ì„±ëŠ¥ ì´ìƒ í˜„ìƒ ë¶„ì„")
        print("="*60)
        
        # ì§€ì—°ì‹œê°„ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ì€ ê²½ìš° ì°¾ê¸°
        normal_latency_threshold = 10.0  # 10Î¼së¥¼ ì„ê³„ê°’ìœ¼ë¡œ ì„¤ì •
        anomalies = self.results_df[self.results_df['avg_latency_us'] > normal_latency_threshold]
        
        if anomalies.empty:
            print("âœ… ì„±ëŠ¥ ì´ìƒ í˜„ìƒ ì—†ìŒ")
            return
            
        print(f"ğŸ” ë°œê²¬ëœ ì´ìƒ í˜„ìƒ: {len(anomalies)}ê±´")
        
        # ì´ìƒ í˜„ìƒ íŒ¨í„´ ë¶„ì„
        anomaly_patterns = anomalies.groupby(['benchmark_type', 'write_buffer_size_mb', 'max_write_buffer_number', 'min_write_buffer_number_to_merge'])['avg_latency_us'].agg(['mean', 'count']).round(1)
        
        print("\nğŸ“‹ ì´ìƒ í˜„ìƒ íŒ¨í„´:")
        for (bench, size, max_buf, min_merge), stats in anomaly_patterns.iterrows():
            print(f"   - {bench} | {size}MB | max_buf={max_buf} | min_merge={min_merge}")
            print(f"     í‰ê·  ì§€ì—°ì‹œê°„: {stats['mean']:.1f}Î¼s (ë°œìƒ íšŸìˆ˜: {stats['count']:.0f})")
        
        # ê°€ëŠ¥í•œ ì›ì¸ ë¶„ì„
        print("\nğŸ”¬ ì›ì¸ ë¶„ì„:")
        
        # 1. í° write_buffer_sizeì™€ ê´€ë ¨ëœ ì´ìƒ
        large_buffer_anomalies = anomalies[anomalies['write_buffer_size_mb'] >= 512]
        if not large_buffer_anomalies.empty:
            print("   1ï¸âƒ£ í° Write Buffer Size (â‰¥512MB) ê´€ë ¨:")
            print("      - ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ìŠ¤ì™‘ ë°œìƒ ê°€ëŠ¥ì„±")
            print("      - ì»´íŒ©ì…˜ ì§€ì—°ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜")
        
        # 2. min_write_buffer_number_to_merge=3ê³¼ ê´€ë ¨ëœ ì´ìƒ
        merge_anomalies = anomalies[anomalies['min_write_buffer_number_to_merge'] == 3]
        if not merge_anomalies.empty:
            print("   2ï¸âƒ£ min_write_buffer_number_to_merge=3 ê´€ë ¨:")
            print("      - ê³¼ë„í•œ ë²„í¼ ëˆ„ì ìœ¼ë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ì••ë°•")
            print("      - ëŒ€ëŸ‰ ì»´íŒ©ì…˜ ì‘ì—…ìœ¼ë¡œ ì¸í•œ I/O ë³‘ëª©")
    
    def generate_recommendations(self):
        """ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        print("\n" + "="*60)
        print("ğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­")
        print("="*60)
        
        # ê¸°ë³¸ ì„¤ì •ì—ì„œì˜ ìµœì  write_buffer_size ì°¾ê¸°
        baseline_summary = self.summary_df[
            (self.summary_df['max_write_buffer_number'] == 2) & 
            (self.summary_df['min_write_buffer_number_to_merge'] == 1)
        ]
        
        recommendations = []
        
        for benchmark in ['fillrandom', 'readrandom', 'overwrite']:
            bench_data = baseline_summary[baseline_summary['benchmark_type'] == benchmark]
            if not bench_data.empty:
                optimal_size = bench_data.loc[bench_data['avg_latency_us_mean'].idxmin(), 'write_buffer_size_mb']
                optimal_latency = bench_data['avg_latency_us_mean'].min()
                recommendations.append((benchmark, optimal_size, optimal_latency))
        
        print("\nğŸ¯ ë²¤ì¹˜ë§ˆí¬ë³„ ìµœì  ì„¤ì •:")
        for benchmark, size, latency in recommendations:
            print(f"   - {benchmark:12s}: write_buffer_size = {size:3.0f}MB (ì§€ì—°ì‹œê°„: {latency:.3f}Î¼s)")
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        print("\nğŸ“‹ ì¼ë°˜ ê¶Œì¥ì‚¬í•­:")
        print("   1ï¸âƒ£ Write Buffer Size:")
        print("      - ëŒ€ë¶€ë¶„ì˜ ì›Œí¬ë¡œë“œì—ì„œ 128MB~256MBê°€ ìµœì ")
        print("      - 512MB ì´ìƒì€ ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì„±ëŠ¥ ì €í•˜ ìœ„í—˜")
        
        print("\n   2ï¸âƒ£ Buffer ê°œìˆ˜ ì„¤ì •:")
        print("      - max_write_buffer_number = 2~4 ê¶Œì¥")
        print("      - min_write_buffer_number_to_merge = 1~2 ê¶Œì¥")
        print("      - min_merge = 3ì€ ì„±ëŠ¥ ì €í•˜ ìœ„í—˜ (ì»´íŒ©ì…˜ ì§€ì—°)")
        
        print("\n   3ï¸âƒ£ ë©”ëª¨ë¦¬ ê³ ë ¤ì‚¬í•­:")
        print("      - ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ = write_buffer_size Ã— max_write_buffer_number")
        print("      - ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ì˜ 25% ì´í•˜ë¡œ ì œí•œ ê¶Œì¥")
        
        print("\n   4ï¸âƒ£ ì›Œí¬ë¡œë“œë³„ ìµœì í™”:")
        print("      - ì“°ê¸° ì§‘ì•½ì : 128MB, max_buffers=4")
        print("      - ì½ê¸° ì§‘ì•½ì : 64MB~128MB, max_buffers=2")
        print("      - í˜¼í•© ì›Œí¬ë¡œë“œ: 128MB, max_buffers=2")
    
    def create_visualizations(self):
        """ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™”"""
        print("\n" + "="*60)
        print("ğŸ“Š ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")
        print("="*60)
        
        # ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('RocksDB Write Buffer ìµœì í™” ì‹¤í—˜ ê²°ê³¼', fontsize=16, fontweight='bold')
        
        # 1. Write Buffer Sizeë³„ ì§€ì—°ì‹œê°„ (ê¸°ë³¸ ì„¤ì •)
        baseline_data = self.results_df[
            (self.results_df['max_write_buffer_number'] == 2) & 
            (self.results_df['min_write_buffer_number_to_merge'] == 1)
        ]
        
        ax1 = axes[0, 0]
        for benchmark in ['fillrandom', 'readrandom', 'overwrite']:
            bench_data = baseline_data[baseline_data['benchmark_type'] == benchmark]
            if not bench_data.empty:
                latency_stats = bench_data.groupby('write_buffer_size_mb')['avg_latency_us'].agg(['mean', 'std'])
                ax1.errorbar(latency_stats.index, latency_stats['mean'], 
                           yerr=latency_stats['std'], marker='o', label=benchmark, linewidth=2)
        
        ax1.set_xlabel('Write Buffer Size (MB)')
        ax1.set_ylabel('í‰ê·  ì§€ì—°ì‹œê°„ (Î¼s)')
        ax1.set_title('Write Buffer Sizeë³„ ì„±ëŠ¥ ë¹„êµ')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_yscale('log')  # ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ì´ìƒì¹˜ í‘œì‹œ
        
        # 2. Buffer ì„¤ì • ì¡°í•©ë³„ ì„±ëŠ¥ (fillrandom, 128MB)
        ax2 = axes[0, 1]
        config_data = self.results_df[
            (self.results_df['write_buffer_size_mb'] == 128) & 
            (self.results_df['benchmark_type'] == 'fillrandom')
        ]
        
        if not config_data.empty:
            pivot_data = config_data.pivot_table(
                values='avg_latency_us', 
                index='max_write_buffer_number', 
                columns='min_write_buffer_number_to_merge', 
                aggfunc='mean'
            )
            
            sns.heatmap(pivot_data, annot=True, fmt='.1f', cmap='YlOrRd', ax=ax2)
            ax2.set_title('Buffer ì„¤ì • ì¡°í•©ë³„ ì§€ì—°ì‹œê°„\n(fillrandom, 128MB)')
            ax2.set_xlabel('min_write_buffer_number_to_merge')
            ax2.set_ylabel('max_write_buffer_number')
        
        # 3. ë²¤ì¹˜ë§ˆí¬ë³„ ì„±ëŠ¥ ë¶„í¬
        ax3 = axes[1, 0]
        normal_data = self.results_df[self.results_df['avg_latency_us'] <= 10.0]  # ì •ìƒ ë²”ìœ„ë§Œ
        
        benchmark_order = ['fillrandom', 'readrandom', 'overwrite']
        box_data = [normal_data[normal_data['benchmark_type'] == b]['avg_latency_us'].values 
                   for b in benchmark_order if not normal_data[normal_data['benchmark_type'] == b].empty]
        
        if box_data:
            ax3.boxplot(box_data, labels=benchmark_order[:len(box_data)])
            ax3.set_ylabel('ì§€ì—°ì‹œê°„ (Î¼s)')
            ax3.set_title('ë²¤ì¹˜ë§ˆí¬ë³„ ì„±ëŠ¥ ë¶„í¬ (ì •ìƒ ë²”ìœ„)')
            ax3.grid(True, alpha=0.3)
        
        # 4. ì´ìƒì¹˜ ë¶„ì„
        ax4 = axes[1, 1]
        anomaly_data = self.results_df[self.results_df['avg_latency_us'] > 10.0]
        
        if not anomaly_data.empty:
            scatter_data = anomaly_data.groupby(['write_buffer_size_mb', 'benchmark_type'])['avg_latency_us'].mean().reset_index()
            
            for benchmark in scatter_data['benchmark_type'].unique():
                bench_scatter = scatter_data[scatter_data['benchmark_type'] == benchmark]
                ax4.scatter(bench_scatter['write_buffer_size_mb'], bench_scatter['avg_latency_us'], 
                          label=benchmark, s=100, alpha=0.7)
            
            ax4.set_xlabel('Write Buffer Size (MB)')
            ax4.set_ylabel('í‰ê·  ì§€ì—°ì‹œê°„ (Î¼s)')
            ax4.set_title('ì„±ëŠ¥ ì´ìƒì¹˜ ë¶„ì„')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        else:
            ax4.text(0.5, 0.5, 'ì´ìƒì¹˜ ì—†ìŒ', ha='center', va='center', transform=ax4.transAxes, fontsize=14)
            ax4.set_title('ì„±ëŠ¥ ì´ìƒì¹˜ ë¶„ì„')
        
        plt.tight_layout()
        
        # ê·¸ë˜í”„ ì €ì¥
        output_path = self.data_dir / "performance_analysis.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ ì‹œê°í™” ê²°ê³¼ ì €ì¥: {output_path}")
        
        # ì¶”ê°€ ìƒì„¸ ë¶„ì„ ê·¸ë˜í”„
        self._create_detailed_charts()
        
        plt.show()
    
    def _create_detailed_charts(self):
        """ìƒì„¸ ë¶„ì„ ì°¨íŠ¸ ìƒì„±"""
        # Write Buffer Sizeë³„ ìƒì„¸ ë¶„ì„
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        fig.suptitle('Write Buffer Size ìƒì„¸ ë¶„ì„', fontsize=16, fontweight='bold')
        
        baseline_data = self.results_df[
            (self.results_df['max_write_buffer_number'] == 2) & 
            (self.results_df['min_write_buffer_number_to_merge'] == 1)
        ]
        
        benchmarks = ['fillrandom', 'readrandom', 'overwrite']
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
        
        for i, (benchmark, color) in enumerate(zip(benchmarks, colors)):
            ax = axes[i]
            bench_data = baseline_data[baseline_data['benchmark_type'] == benchmark]
            
            if not bench_data.empty:
                # ê°œë³„ ë°ì´í„° í¬ì¸íŠ¸
                for size in bench_data['write_buffer_size_mb'].unique():
                    size_data = bench_data[bench_data['write_buffer_size_mb'] == size]['avg_latency_us']
                    ax.scatter([size] * len(size_data), size_data, alpha=0.6, color=color, s=30)
                
                # í‰ê· ì„ 
                latency_stats = bench_data.groupby('write_buffer_size_mb')['avg_latency_us'].mean()
                ax.plot(latency_stats.index, latency_stats.values, 'o-', color=color, linewidth=2, markersize=8)
            
            ax.set_xlabel('Write Buffer Size (MB)')
            ax.set_ylabel('ì§€ì—°ì‹œê°„ (Î¼s)')
            ax.set_title(f'{benchmark.upper()}')
            ax.grid(True, alpha=0.3)
            
            # ì´ìƒì¹˜ê°€ ìˆëŠ” ê²½ìš° ë¡œê·¸ ìŠ¤ì¼€ì¼ ì ìš©
            if bench_data['avg_latency_us'].max() > 50:
                ax.set_yscale('log')
        
        plt.tight_layout()
        
        # ìƒì„¸ ë¶„ì„ ê·¸ë˜í”„ ì €ì¥
        output_path = self.data_dir / "detailed_analysis.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ìƒì„¸ ë¶„ì„ ì €ì¥: {output_path}")
    
    def generate_report(self):
        """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*60)
        print("ğŸ“ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±")
        print("="*60)
        
        report_path = self.data_dir / "analysis_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# RocksDB Write Buffer ìµœì í™” ì‹¤í—˜ ë¶„ì„ ë³´ê³ ì„œ\n\n")
            f.write("## ì‹¤í—˜ ê°œìš”\n")
            f.write(f"- ì´ ì‹¤í—˜ íšŸìˆ˜: {len(self.results_df)}\n")
            f.write(f"- í…ŒìŠ¤íŠ¸ëœ ì„¤ì • ì¡°í•©: {len(self.summary_df)}\n")
            f.write("- ë²¤ì¹˜ë§ˆí¬: fillrandom, readrandom, overwrite\n")
            f.write("- Write Buffer Size: 16MB, 64MB, 128MB, 256MB, 512MB\n\n")
            
            # ì£¼ìš” ë°œê²¬ì‚¬í•­
            f.write("## ì£¼ìš” ë°œê²¬ì‚¬í•­\n\n")
            
            # ìµœì  ì„±ëŠ¥ ì„¤ì •
            baseline_summary = self.summary_df[
                (self.summary_df['max_write_buffer_number'] == 2) & 
                (self.summary_df['min_write_buffer_number_to_merge'] == 1)
            ]
            
            f.write("### 1. ìµœì  Write Buffer Size\n")
            for benchmark in ['fillrandom', 'readrandom', 'overwrite']:
                bench_data = baseline_summary[baseline_summary['benchmark_type'] == benchmark]
                if not bench_data.empty:
                    optimal_size = bench_data.loc[bench_data['avg_latency_us_mean'].idxmin(), 'write_buffer_size_mb']
                    optimal_latency = bench_data['avg_latency_us_mean'].min()
                    f.write(f"- **{benchmark}**: {optimal_size:.0f}MB (í‰ê·  ì§€ì—°ì‹œê°„: {optimal_latency:.3f}Î¼s)\n")
            
            # ì„±ëŠ¥ ì´ìƒ í˜„ìƒ
            f.write("\n### 2. ì„±ëŠ¥ ì´ìƒ í˜„ìƒ\n")
            anomalies = self.results_df[self.results_df['avg_latency_us'] > 10.0]
            if not anomalies.empty:
                f.write(f"- ì´ {len(anomalies)}ê±´ì˜ ì„±ëŠ¥ ì´ìƒ í˜„ìƒ ë°œê²¬\n")
                f.write("- ì£¼ìš” ì›ì¸:\n")
                f.write("  - 512MB ì´ìƒì˜ í° Write Buffer Size\n")
                f.write("  - min_write_buffer_number_to_merge=3 ì„¤ì •\n")
                f.write("  - ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ìŠ¤ì™‘ ë°œìƒ ì¶”ì •\n")
            else:
                f.write("- ì„±ëŠ¥ ì´ìƒ í˜„ìƒ ì—†ìŒ\n")
            
            # ê¶Œì¥ì‚¬í•­
            f.write("\n## ìµœì í™” ê¶Œì¥ì‚¬í•­\n\n")
            f.write("### ì¼ë°˜ì ì¸ ì„¤ì •\n")
            f.write("- **write_buffer_size**: 128MB~256MB\n")
            f.write("- **max_write_buffer_number**: 2~4\n")
            f.write("- **min_write_buffer_number_to_merge**: 1~2\n\n")
            
            f.write("### ì›Œí¬ë¡œë“œë³„ ìµœì í™”\n")
            f.write("- **ì“°ê¸° ì§‘ì•½ì **: write_buffer_size=128MB, max_write_buffer_number=4\n")
            f.write("- **ì½ê¸° ì§‘ì•½ì **: write_buffer_size=64MB~128MB, max_write_buffer_number=2\n")
            f.write("- **í˜¼í•© ì›Œí¬ë¡œë“œ**: write_buffer_size=128MB, max_write_buffer_number=2\n\n")
            
            f.write("### ì£¼ì˜ì‚¬í•­\n")
            f.write("- 512MB ì´ìƒì˜ write_buffer_sizeëŠ” ì„±ëŠ¥ ì €í•˜ ìœ„í—˜\n")
            f.write("- min_write_buffer_number_to_merge=3ì€ ì»´íŒ©ì…˜ ì§€ì—° ë°œìƒ\n")
            f.write("- ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ì˜ 25%ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜\n")
        
        print(f"ğŸ“„ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {report_path}")
    
    def run_complete_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ RocksDB Write Buffer ìµœì í™” ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ì‹œì‘")
        print("="*60)
        
        # ê° ë¶„ì„ ë‹¨ê³„ ì‹¤í–‰
        self.analyze_write_buffer_size_impact()
        self.analyze_buffer_configuration_impact()
        self.identify_performance_anomalies()
        self.generate_recommendations()
        self.create_visualizations()
        self.generate_report()
        
        print("\n" + "="*60)
        print("âœ… ë¶„ì„ ì™„ë£Œ!")
        print("ğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print("   - performance_analysis.png: ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ì°¨íŠ¸")
        print("   - detailed_analysis.png: ìƒì„¸ ë¶„ì„ ì°¨íŠ¸")
        print("   - analysis_report.md: ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ")
        print("="*60)

if __name__ == "__main__":
    # ë¶„ì„ ì‹¤í–‰
    analyzer = RocksDBAnalyzer()
    analyzer.run_complete_analysis() 